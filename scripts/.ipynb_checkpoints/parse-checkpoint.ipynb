{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8004c449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 11px;\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style type='text/css'>\n",
    ".CodeMirror{\n",
    "font-size: 11px;\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50523044",
   "metadata": {},
   "source": [
    ".pkl to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a74c9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def ncols(page):\n",
    "    max_cols = 0\n",
    "    for _, row in page.iterrows():\n",
    "        ncols = len(row[~pd.isna(row)])\n",
    "        max_cols = max(max_cols, ncols)\n",
    "    return ncols\n",
    "\n",
    "def get_max_cols(lst):\n",
    "    return max([ncols(page) for page in tqdm(lst)])\n",
    "\n",
    "def parse_doses(row):\n",
    "    '''\n",
    "    Return dict mapping drug -> doseage\n",
    "    '''\n",
    "    def canonical_str(col, doses_dict):\n",
    "        '''\n",
    "        Col name: {DRUG}+{DRUG}+...+{DRUG}\n",
    "        Return: {DRUG}{DOSE}{DRUG}{DOSE}...\n",
    "        '''\n",
    "        parts = col.split('+')\n",
    "        return '-'.join([p + str(doses_dict[p]) for p in parts])\n",
    "\n",
    "    doses = {}\n",
    "    s = row[0]\n",
    "    parts = [s[i:i+4] for i in range(0, len(s), 4)]\n",
    "\n",
    "    for p in parts:\n",
    "        doses[p[:3]] = int(p[-1])\n",
    "\n",
    "    cols = []\n",
    "    for cstr in row[1:]:\n",
    "        if type(cstr) == str:\n",
    "            for c in cstr.split(' '):\n",
    "                cols.append(canonical_str(c, doses))\n",
    "        else:\n",
    "            continue\n",
    "    assert len(cols) == 31\n",
    "    return cols\n",
    "\n",
    "def clean_row(row):\n",
    "    values = []\n",
    "    for v in row.values:\n",
    "        if type(v) == str:\n",
    "            for vs in v.split(' '):\n",
    "                try:\n",
    "                    values.append(float(vs))\n",
    "                except:\n",
    "                    if 'TMP' in vs:\n",
    "                        values.append(float(vs[:6]))\n",
    "                    else:\n",
    "                        print(f\"Cant split string in clean row at page {pid}, row {i}\")\n",
    "                        pdb.set_trace()\n",
    "        else:\n",
    "            if not pd.isna(v):\n",
    "                values.append(v)\n",
    "    return np.array(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48175e85",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "The `.isalpha()` method returns True if all the characters are alphabet letters (a-z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a439580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('efvr'.isalpha())\n",
    "print('e4fvr'.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872f13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(lst):\n",
    "    '''\n",
    "    page: DataFrame of current page\n",
    "    prev_page: previous page\n",
    "    '''\n",
    "    # loop over a page\n",
    "    cols = None\n",
    "    index = []\n",
    "    batched_values = []\n",
    "    col_vals = []\n",
    "\n",
    "    for pid, pg in tqdm(enumerate(lst)):\n",
    "        print(\"page #:\", pid+1)\n",
    "        #print(pg)\n",
    "        # need access to last true row of lst\n",
    "        for i, row in pg.iterrows():\n",
    "            #print(\"row index of the dataframe:\", i)\n",
    "            cleaned_row = row[~pd.isna(row)]\n",
    "            try:\n",
    "                if (type(cleaned_row.values[0]) != float) and cleaned_row.values[0][0].isalpha(): # start of a label row\n",
    "                    x = 22\n",
    "            except:\n",
    "                print(f\"Cant parse doses at page {pid}, row {i}\")\n",
    "                pdb.set_trace() # setting breakpoints and single stepping at the source line level for debugging.\n",
    "            if (type(cleaned_row.values[0]) != float) and cleaned_row.values[0][0].isalpha(): # start of a label row\n",
    "                # evict an l thing \n",
    "                if len(col_vals) > 0:\n",
    "                    #print(\"col_vals: \", len(col_vals))\n",
    "                    #print(\"COlS: ---\", cols) # label row\n",
    "                    if cols != None:\n",
    "                        #print(len(cols))\n",
    "                        index.extend(cols)\n",
    "                    batch = np.stack(col_vals).T\n",
    "                    #print(\"batch shape: \",batch.shape)\n",
    "                    assert batch.shape[0] == 31, \"Incorrect batch shape! {}\".format(batch.shape)\n",
    "                    res = np.zeros((batch.shape[0], 5))\n",
    "                    #print(\"res : \",res)\n",
    "                    res[:, :batch.shape[1]] = batch\n",
    "                    batched_values.append(res)\n",
    "                    #batched_values.append(batch)\n",
    "                    col_vals = []\n",
    "\n",
    "                try:\n",
    "                    cols = parse_doses(cleaned_row)\n",
    "                except:\n",
    "                    print(f\"Cant parse doses at page {pid}, row {i}\")\n",
    "                    pdb.set_trace()\n",
    "            else:\n",
    "                try:\n",
    "                    col_vals.append(clean_row(cleaned_row)) #cleaned_row.values.astype(float))\n",
    "                except:\n",
    "                    print(f\"Cant clean row at page {pid}, row {i}\")\n",
    "                    pdb.set_trace()\n",
    "\n",
    "    if len(col_vals) > 0:\n",
    "            index.extend(cols)\n",
    "            batch = np.stack(col_vals).T\n",
    "            assert (batch.shape[0] == 31), \"Incorrect batch shape!\"\n",
    "            res = np.zeros((batch.shape[0], 5))\n",
    "            res[:, :batch.shape[1]] = batch\n",
    "            batched_values.append(res)\n",
    "            col_vals = []\n",
    "\n",
    "    values = np.vstack(batched_values)\n",
    "    #print(type(index))\n",
    "    \n",
    "    aprox_missing = abs(values.shape[0]-len(index))\n",
    "    #print(index[-aprox_missing:])\n",
    "    index.extend(index[-aprox_missing:])\n",
    "    df = pd.DataFrame(values, index=index)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdf229a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './area_pdf.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gh/3pd23sd51hn2p7774mv2zt4r0000gn/T/ipykernel_6086/2481536144.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mparsed_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./drug_all.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/gh/3pd23sd51hn2p7774mv2zt4r0000gn/T/ipykernel_6086/2481536144.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./area_pdf.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#f = open('./small.pkl', 'rb')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mparsed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './area_pdf.pkl'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    f = open('./area_pdf.pkl', 'rb')\n",
    "    #f = open('./small.pkl', 'rb')\n",
    "    lst = pickle.load(f)\n",
    "    parsed_df = parse(lst)\n",
    "    print(parsed_df.shape)\n",
    "    parsed_df.to_csv('./drug_all.csv')\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --force-reinstall pandas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
